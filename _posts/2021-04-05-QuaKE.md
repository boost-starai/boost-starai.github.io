---
layout: splash
permalink: /papers/QuaKE/
title: "A Probabilistic Approach to Extract Qualitative Knowledge for Early Prediction of Gestational Diabetes"
categories: paper
description: 'by Athresh Karanam, Alexander L. Hayes, Harsha Kokel, David M. Haas, Predrag Radivojac, Sriraam Natarajan, In AIME 2021'
excerpt: '<i>Athresh Karanam, Alexander L. Hayes, Harsha Kokel, David M. Haas, Predrag Radivojac, Sriraam Natarajan</i><br/><br/>{::nomarkdown}  <a href="/assets/pdfs/KaranamHayes_AIME2021.pdf" class="btn btn--light-outline btn--large"><i class="fas fa-file-pdf"></i> Paper</a>  <a href="/assets/pdfs/KaranamHayes_AIME2021_sup.pdf" class="btn btn--light-outline btn--large"><i class="fas fa-paperclip"></i> Appendix</a>{:/nomarkdown}'
header:
  overlay_color: SteelBlue  
---

## Quick Overview

**Accepted** at the 19th International Conference on Artificial Intelligence in
Medicine ([AIME 2021](https://aime21.aimedicine.info/)), and published in
Springer Lecture Notes in Artificial Intelligence.

In this work: (**1**) we developed an algorithm to
extract qualitative rules from a causal/probabilistic model,
(**2**) we applied our algorithm to a data set for predicting gestational
diabetes from clinical observations, and
(**3**) we verified the rules with the prior knowledge of a clinical expert in
gynecology (Dr. Haas)&mdash;finding a precision of 0.923 and suggesting some
mismatched cases could be future research directions.

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$']]},
    extensions: [
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    jax: ["input/TeX", "output/CommonHTML"],
    TeX: {
      extensions: [
        "AMSmath.js",
        "AMSsymbols.js",
        "noErrors.js",
        "noUndefined.js",
      ]
    }
  });
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## Qualitative Knowledge in Machine Learning

This work combines two project categories our group investigates:
"Human-Allied Artificial Intelligence" and "Precision Health."

Consider the following statements:

- Risk of lung cancer increases with the number of cigarettes smoked
- Risk of high blood glucose decreases with each additional hour of sleep,
  up to eight
- Risk of gestational diabetes (GDM) increases with BMI

Each statement describes how a change in one variable causes influences
the change in another variable. Such statements are a common result of
medical studies, and may be highly influential in medical decision making.
These are statements about **monotonic influence** (MI) between variables,
denoted by $X_{\prec}^{M+}Y$.
Previous work asked domain experts to provide these statements up front,
and empirical results show that this inductive bias makes it possible
to learn good models even in settings with little data.[^2]

Now consider these statements:

- Increase in BMI increases the risk of high blood pressure in patients
  family history of hypertension more than in patients without such
  family history.
- Increase in blood sugar level increases the risk of heart attack in high
  cholesterol level patients more than it does in patients with low
  cholesterol levels.[^4]

These are statements about **synergistic influnece** (SI) between sets of
variables ${A,B}_{\prec}^{S+}Y$.

Here we are interested in the reverse problem: given a causal/probabilistic
world model, can we reverse-engineer the qualitative knowledge that experts
have? This has potential to be an important component for getting machines to reason and
explain themselves in ways that are amenable to how humans understand the world.
But even though there has been sizable investigation into learning with human
knowledge in the form of qualitative statements, we know of no work that has
done the reverse.

## Rule mining doesn't handle this case

*Decision Rules* are close in spirit.[^1] Assuming data is ordinal,
categorical, or binned from continuous attributes; a rule such as the following
could be observed:

>
$\text{if}~(X_{0} = 0 \land X_{1} = 0);~\text{then}$   
$\quad Y = 0$

Substituting human-readable names for the variables:

>
$\text{if}~(\text{Systolic Blood Pressure Category} = 0 \land \text{Diastolic Blood Pressure Category} = 0);~\text{then}$   
$\quad \text{Blood Pressure Category} = 0$

And there's a correspondence between the rule statements and natural language:
Which can be written as a statement describing blood pressure categories:

$$
\text{If a person's systolic blood pressure is less than 120 and their}\\
\text{diastolic blood pressure is less than 80, then they have normal blood pressure.}
$$

But how should one write a statement from earlier:
"*Risk of gestational diabetes (GDM) increases with BMI*"
as a conjunctive rule? It is not obvious. Expressing this statement
requires expressing all possible values of the body mass index
(BMI) variable, expressing the possible values of gestational diabetes,
weighing the rules by the probability of each outcome, and finally
asserting that a monotonic increase in the former implies a monotonic
increase in the latter.[^3]

We express this rule using our notation from earlier:

$$
\text{BMI}_{\prec}^{M+}\text{GDM}
$$

This is a much more general statement that concerns all possible values of
both variables, and describes how BMI influences the risk of GDM. This
single rule concisely expresses an idea that it would take multiple
decision rules.

## Learning Qualitative Influence Statements

We develop metrics to measure the degree of monotonicity and synergy,
denoted by $\delta_{a}$ for *degree of monotonic influence* and
$\delta_{a,b}$ for *degree of synergistic influence*.

*Degree of monotonic influence* $\delta_{a}$ of a variable
$X_{a} \in \boldsymbol{X}$ on $Y$ is defined as:

$$
\delta_{a} = I_{(C_a>0)} \cdot \sum_j\sum_{j'>j}\sum_k \frac{P(Y \leq k|X_a=x_a^j) - P(Y \leq k|X_a=x_a^{j'})}{|X_a|}
$$

where,

$$
C_{a} = \prod_{j}\prod_{j'>j}\prod_{k}{\max(P(Y \leq k|X_a=x_a^j) - P(Y \leq k|X_a=x_a^{j'}) + \epsilon_m, 0)}
$$

*Degree of synergistic influence* is defined in a similar way:

$$
\delta_{a,b} = I_{(C_{a,b}>0)} \cdot \sum_i\sum_{i'>i}\sum_j\sum_{j'>j} \frac{\phi_{a,b}^{i,i',j,j'}}{|X_a| \cdot |X_b|}
$$

where,

$$
C_{a,b} = \prod_i\prod_{i'>i}\prod_j\prod_{j'>j} \max(\phi_{a,b}^{i,i',j,j'} + \epsilon_s,0)
$$

and

$$
\begin{split}
\phi_{a,b}^{i,i',j,j'} = \sum_k P(Y \leq k|X_a=x_a^i,X_b=x_b^j) &- P(Y \leq k|X_a=x_a^{i'},X_b=x_b^j)~- \\
    P(Y \leq k|X_a=x_a^{i}, X_b=x_b^{j'}) &+ P(Y \leq k|X_a=x_a^{i'}, X_b=x_b^{j'})
\end{split}
$$

### The Qualitative Knowledge Extraction (QuaKE) Algorithm

> **Input**:   
$\quad$ joint probability model $P(Y, \boldsymbol{X})$,   
$\quad$ label vector $Y$, ordinal feature vector $\boldsymbol{X}$,   
$\quad$ monotonic and synergistic slack parameters: $\epsilon_{m}$, $\epsilon_{s}$   
$\quad$ monotonic and synergistic threshold parameters $T_{m}$, $T_{s}$.   
> **Initialize**: $\boldsymbol{R} \leftarrow \emptyset$
>
> **for** $a \leftarrow 0$ to $(|\boldsymbol{X}| - 1)$ **do**   
$\quad$ compute $\delta_{a}$ using (1)   
$\quad$ **if** $\delta_{a} \geq T_{m}$ **then**   
$\qquad$ $\boldsymbol{R} \leftarrow$ ($X_{a\prec}^{M+}Y$) $\cup$ $\boldsymbol{R}$   
$\quad$ **for** $b \leftarrow a + 1$ **to** $(|\boldsymbol{X}| - 1)$ **do**   
$\qquad$ compute $\delta_{a,b}$ using (3)   
$\qquad$ **if** $\delta_{a,b} \geq T_{s}$ **then**   
$\qquad \quad$ $\boldsymbol{R}$ $\leftarrow$ (${X_a,X_b}_{\prec}^{S+}Y$) $\cup$ $\boldsymbol{R}$   
**return** $\boldsymbol{R}$

## Citation

If you build on or use portions of this work, please provide credit using the following MLA or BibTeX citations.

Athresh Karanam, Alexander L. Hayes, Harsha Kokel, David M. Haas, Predrag Radivojac, and Sriraam Natarajan. (2021) *A Probabilistic Approach to Extract Qualitative Knowledge for Early Prediction of Gestational Diabetes*

```bibtex
@inproceedings{karanam2021quake,
  authors = {Athresh Karanam and Alexander L. Hayes and Harsha Kokel and David M. Haas and Predrag Radivojac and Sriraam Natarajan},
  title = {A Probabilistic Approach to Extract Qualitative Knowledge for Early Prediction of Gestational Diabetes},
  year = {2021},
}
```

## Footnotes

[^1]: This example is concise to a fault. Decision rules and decisions trees are often popular since their are inherently interpretable. For a longer discussion on learning decision rules, Christoph Molnar's summary can be a good place to look: [Interpretable Machine Learning > Interpretable Models > Decision Rules](https://christophm.github.io/interpretable-ml-book/rules.html).

[^2]: Altendorf et al. presented results of monotonic constraints that included the [UCI Breast Cancer Wisconsin](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)) data set as an extreme case of this, scoring 90% accuracy with a single example and qualitative background knowledge in the form of monotonicities---this showed that an informed learner with a single example could be equivalent to an uninformed learner given hundreds of examples. For full details, see learning curves in Figure 8 of "[Learning from Sparse Data by Exploiting Monotonicity Constraints](https://arxiv.org/abs/1207.1364)."

[^3]: We speculated that this could still be a viable option. Gopalakrishnan 2010 studied a related problem and proposed a "*Bayesian Rule Extraction*" algorithm. This learned the structure and parameters of a Bayesian Network (BN) then extracted rules by comparing confidence factors between binary outcomes conditioned on the same evidence. It may be possible to extract qualitative rules by running the Gopalakrishnan algorithm on learned BNs and checking whether the confidence factors increase (decrease) as the factors increase (decrease). For full details, refer to their paper on "[Bayesian rule learning for biomedical data mining](https://doi.org/10.1093/bioinformatics/btq005)." Alexander L. Hayes implemented a version of this, an overview and example of applying the method is included in the [README of this GitHub repository](https://github.com/hayesall/bn-rule-extraction).

[^4]: From Yang and Natarajan (2013) "[Knowledge Intensive Learning: Combining Qualitative Constraints with Causal Independence for Parameter Learning in Probabilistic Models](https://starling.utdallas.edu/assets/pdfs/KIL_ECML13.pdf)"
